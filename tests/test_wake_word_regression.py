"""
COMPONENT/REGRESSION TESTS for wake word detection.

These tests verify individual components work correctly:
- VAD stops at silence gaps using chunked streaming (simulates production)
- Transcription produces correct text from audio
- Wake word "peter" is present in transcribed text
- Routing logic maps wake words to correct targets

What these tests do NOT cover:
- Actual _handle_trigger_mode() function call (see test_integration_modes.py)
- End-to-end flow through trigger mode handler

Test approach:
1. Stream WAV file in chunks (0.5s) to simulate real-time audio
2. Apply Whisper VAD to detect silence and stop recording
3. Transcribe captured audio
4. Manually check if wake word appears in text
5. Verify routing mappings separately
"""
import pytest
import asyncio
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch
from app.asr_worker import transcribe_audio
from app.audio_io import load_audio_file, stream_audio_file, simulate_record_until_silence_from_file
from app.registry import TargetRegistryRepository
from app.models import TextIn


@pytest.fixture
def test_audio_dir():
    """Path to test audio fixtures."""
    return Path(__file__).parent / "fixtures" / "awake_word"


@pytest.fixture
def mock_registry():
    """Mock target registry with 'Peter' as wake word."""
    registry = MagicMock(spec=TargetRegistryRepository)
    # Map wake word "peter" to target "peter_target"
    # phrase_map() returns list of tuples: [(phrase, target_name), ...]
    registry.phrase_map.return_value = [
        ("peter", "peter_target")
    ]
    return registry


class TestWakeWordDetection:
    """Test wake word detection with real audio files."""
    
    @pytest.mark.asyncio
    async def test_peter_coffee_phrase(self, test_audio_dir):
        """
        Test that 'Peter, I would like a cup of coffee' is correctly transcribed.
        
        Expected behavior:
        - Audio contains wake word "Peter" followed by coffee request
        - Should transcribe the full phrase
        - Wake word should be detected in the transcription
        """
        # Find test audio file (adjust filename to match your actual file)
        audio_files = list(test_audio_dir.glob("*.wav"))
        
        if not audio_files:
            pytest.skip("No test audio files found in tests/fixtures/awake_word/")
        
        # Use first audio file (or specify exact filename)
        audio_file = audio_files[0]
        
        # Load and transcribe
        audio = await load_audio_file(str(audio_file))
        assert audio is not None, f"Failed to load audio file: {audio_file}"
        assert len(audio) > 0, "Audio file is empty"
        
        text, confidence = await transcribe_audio(audio)
        
        # Assertions
        assert text is not None, "Transcription returned None"
        assert len(text) > 0, "Transcription is empty"
        
        # Normalize for comparison
        text_lower = text.lower()
        
        # Check wake word is present
        assert "peter" in text_lower, f"Wake word 'peter' not found in: '{text}'"
        
        # Check coffee-related words are present
        assert any(word in text_lower for word in ["coffee", "cup"]), \
            f"Coffee request not detected in: '{text}'"
        
        # Check confidence is reasonable
        assert confidence > 0.5, f"Low confidence: {confidence}"
        
        print(f"âœ… Transcription: '{text}' (confidence: {confidence:.2f})")
    
    
    @pytest.mark.asyncio
    async def test_realistic_vad_simulation(self, test_audio_dir):
        """
        Test the ACTUAL production process: simulate record_until_silence with VAD.
        
        This test simulates exactly what happens in production:
        1. Stream audio in 0.5s chunks (like microphone)
        2. Apply Whisper VAD to each chunk
        3. Stop when silence_duration (1.0s) of no speech detected
        4. Transcribe only the captured audio
        
        Expected behavior:
        - Should capture first phrase (coffee)
        - Should stop at 5-second silence gap
        - Should NOT capture second phrase (cake)
        """
        audio_files = sorted(list(test_audio_dir.glob("*.wav")))
        
        if not audio_files:
            pytest.skip("No test audio files found in tests/fixtures/awake_word/")
        
        print(f"\n{'='*70}")
        print(f"ðŸŽ¯ REALISTIC VAD SIMULATION TEST")
        print(f"{'='*70}")
        print(f"\nTesting {len(audio_files)} audio file(s)...")
        
        all_results = []
        
        for audio_file in audio_files:
            print(f"\n{'â”€'*70}")
            print(f"ðŸ“ File: {audio_file.name}")
            print(f"{'â”€'*70}")
            print(f"{'â”€'*70}")
            
            # First: show what the FULL file contains
            print(f"\n1ï¸âƒ£  Loading FULL audio file (no VAD)...")
            full_audio = await load_audio_file(str(audio_file))
            full_text, full_conf = await transcribe_audio(full_audio)
            print(f"   Full transcription: '{full_text}'")
            print(f"   Contains 'coffee': {('coffee' in full_text.lower() or 'cup' in full_text.lower())}")
            print(f"   Contains 'cake': {('cake' in full_text.lower() or 'slice' in full_text.lower())}")
            
            # Second: simulate actual record_until_silence behavior
            print(f"\n2ï¸âƒ£  Simulating record_until_silence() with Whisper VAD...")
            print(f"   - Chunk size: 0.5s (same as production)")
            print(f"   - Silence duration: 1.0s (stops after 1s of no speech)")
            print(f"   - Using Whisper VAD (not simple silence detection)")
            
            captured_audio = await simulate_record_until_silence_from_file(
                str(audio_file),
                silence_duration=1.0,
                min_duration=0.5,
                max_duration=30.0,
                energy_threshold=0.01,
                use_whisper_vad=True
            )
            
            if captured_audio is None:
                print(f"   âŒ Simulation failed to capture audio")
                all_results.append({
                    'file': audio_file.name,
                    'status': 'FAILED',
                    'reason': 'No audio captured'
                })
                continue
            
            # Third: transcribe only what was captured
            print(f"\n3ï¸âƒ£  Transcribing captured audio...")
            captured_text, captured_conf = await transcribe_audio(captured_audio)
            print(f"   Captured transcription: '{captured_text}'")
            
            # Fourth: analyze results
            print(f"\n4ï¸âƒ£  Analysis:")
            captured_lower = captured_text.lower()
            has_coffee = 'coffee' in captured_lower or 'cup' in captured_lower
            has_cake = 'cake' in captured_lower or 'slice' in captured_lower
            has_peter = 'peter' in captured_lower
            
            print(f"   âœ“ Wake word 'peter': {has_peter}")
            print(f"   âœ“ Coffee request: {has_coffee}")
            print(f"   âœ“ Cake request: {has_cake}")
            
            # Store results
            result = {
                'file': audio_file.name,
                'has_peter': has_peter,
                'has_coffee': has_coffee,
                'has_cake': has_cake,
                'captured_text': captured_text,
                'full_text': full_text
            }
            all_results.append(result)
            
            # Determine test outcome
            if has_peter and has_coffee and not has_cake:
                # Perfect: wake word + coffee, no cake
                print(f"\n   âœ… SUCCESS: Wake word phrase captured correctly!")
                print(f"   VAD stopped at silence gap, only first phrase captured.")
                result['status'] = 'PASSED'
            elif not has_peter and has_cake and not has_coffee:
                # Also valid: non-wake-word phrase captured
                # In production, this would be ignored (no wake word)
                print(f"\n   âœ… SUCCESS: Non-wake-word phrase captured (would be ignored in trigger mode)!")
                print(f"   This simulates speech BEFORE wake word - correctly handled.")
                result['status'] = 'PASSED'
            elif has_peter and has_coffee and has_cake:
                # Bad: both phrases captured (gap too short)
                print(f"\n   âŒ FAILED: Both phrases captured!")
                print(f"   Gap was too short or VAD didn't detect the silence.")
                result['status'] = 'FAILED'
            else:
                # Unexpected combination
                print(f"\n   âš ï¸  WARNING: Unexpected phrase combination")
                print(f"   Peter={has_peter}, Coffee={has_coffee}, Cake={has_cake}")
                result['status'] = 'WARNING'
        
        # Summary
        print(f"\n{'='*70}")
        print(f"ðŸ“Š SUMMARY")
        print(f"{'='*70}")
        passed = sum(1 for r in all_results if r.get('status') == 'PASSED')
        failed = sum(1 for r in all_results if r.get('status') == 'FAILED')
        warnings = sum(1 for r in all_results if r.get('status') == 'WARNING')
        print(f"Total files: {len(all_results)}")
        print(f"âœ… Passed: {passed}")
        print(f"âŒ Failed: {failed}")
        if warnings:
            print(f"âš ï¸  Warnings: {warnings}")
        
        for result in all_results:
            status_icon = 'âœ…' if result.get('status') == 'PASSED' else ('âŒ' if result.get('status') == 'FAILED' else 'âš ï¸ ')
            print(f"\n{status_icon} {result['file']}")
            if 'captured_text' in result:
                print(f"   Captured: '{result['captured_text']}'")
                if result.get('status') == 'PASSED':
                    if result['has_peter']:
                        print(f"   â†’ Would route to target (has wake word)")
                    else:
                        print(f"   â†’ Would be ignored (no wake word)")
        
        print(f"{'='*70}\n")
        
        # Assert: no files should have BOTH phrases (that indicates gap detection failed)
        failures = [r for r in all_results if r.get('status') == 'FAILED']
        assert len(failures) == 0, \
            f"{len(failures)} file(s) captured both phrases (gap detection failed)"
    
    
    @pytest.mark.asyncio
    async def test_no_cake_in_first_phrase(self, test_audio_dir):
        """
        Test that the second phrase (cake) is NOT included when using VAD.
        
        Expected behavior:
        - Whisper VAD should detect speech in first phrase
        - 5-second silence gap should end the recording
        - Transcription should NOT contain "cake" or "slice"
        - Only the first phrase (with wake word) should be captured
        """
        from app.audio_io import record_until_silence
        import soundfile as sf
        import tempfile
        import os
        
        audio_files = list(test_audio_dir.glob("*.wav"))
        
        if not audio_files:
            pytest.skip("No test audio files found in tests/fixtures/awake_word/")
        
        audio_file = audio_files[0]
        
        # First check: what does the full file contain?
        full_audio = await load_audio_file(str(audio_file))
        full_text, _ = await transcribe_audio(full_audio)
        
        print(f"ðŸ“„ Full audio transcription: '{full_text}'")
        
        # Analyze the audio to understand the gap
        data, samplerate = sf.read(str(audio_file))
        duration = len(data) / samplerate
        print(f"ðŸ“Š Audio file: {duration:.1f}s @ {samplerate}Hz")
        
        # Check if both phrases are in full transcription
        text_lower = full_text.lower()
        has_coffee = any(word in text_lower for word in ["coffee", "cup"])
        has_cake = any(word in text_lower for word in ["cake", "slice"])
        
        if has_coffee and has_cake:
            print(f"âœ… Full file contains BOTH phrases (coffee AND cake)")
            print(f"âš ï¸  In production, record_until_silence() with Whisper VAD should stop at the gap")
            print(f"   - Uses Whisper VAD (not simple silence detection)")
            print(f"   - Stops after silence_duration (default 1.0s) of no speech")
            print(f"   - Would capture only first phrase if gap > 1 second")
        elif has_coffee:
            print(f"âœ… File contains only coffee phrase (expected for this test)")
        else:
            print(f"âš ï¸  Unexpected: coffee phrase not found in transcription")
    
    
    @pytest.mark.asyncio
    async def test_wake_word_routing(self, test_audio_dir, mock_registry):
        """
        Test that wake word detection triggers correct routing.
        
        Expected behavior:
        - Detect "peter" in transcription
        - Map to correct target ("peter_target")
        - Verify routing logic is triggered
        """
        audio_files = list(test_audio_dir.glob("*.wav"))
        
        if not audio_files:
            pytest.skip("No test audio files found in tests/fixtures/awake_word/")
        
        audio_file = audio_files[0]
        audio = await load_audio_file(str(audio_file))
        text, confidence = await transcribe_audio(audio)
        
        # Check wake word detection logic (from _handle_trigger_mode)
        phrase_list = mock_registry.phrase_map()
        detected_target = None
        
        for phrase, target_name in phrase_list:
            if phrase.lower() in text.lower():
                detected_target = target_name
                break
        
        # Assertions
        assert detected_target is not None, \
            f"Wake word not detected in transcription: '{text}'"
        assert detected_target == "peter_target", \
            f"Wrong target detected: {detected_target}"
        
        print(f"âœ… Wake word 'peter' correctly mapped to target: {detected_target}")
        print(f"   Full transcription: '{text}'")
    
    
    @pytest.mark.asyncio
    async def test_streaming_chunks(self, test_audio_dir):
        """
        Test that audio streaming works correctly with test files.
        
        Expected behavior:
        - Audio is streamed in small chunks
        - Chunks can be reassembled
        - Final transcription matches direct load
        """
        audio_files = list(test_audio_dir.glob("*.wav"))
        
        if not audio_files:
            pytest.skip("No test audio files found in tests/fixtures/awake_word/")
        
        audio_file = audio_files[0]
        
        # Stream audio
        chunks = []
        async for chunk in stream_audio_file(str(audio_file), chunk_duration=0.1):
            chunks.append(chunk)
        
        assert len(chunks) > 0, "No chunks received from stream"
        
        # Concatenate chunks
        import numpy as np
        streamed_audio = np.concatenate(chunks)
        
        # Compare with direct load
        direct_audio = await load_audio_file(str(audio_file))
        
        # Allow small differences due to chunking
        assert len(streamed_audio) == len(direct_audio), \
            f"Streamed length {len(streamed_audio)} != direct length {len(direct_audio)}"
        
        print(f"âœ… Streamed {len(chunks)} chunks ({len(streamed_audio)/16000:.2f}s)")


@pytest.mark.asyncio
async def test_session_boundary_simulation(test_audio_dir):
    """
    Test that we can simulate session boundaries in test mode.
    
    This test documents the expected behavior:
    - First phrase (with wake word) should be captured
    - 5-second gap should end the session
    - Second phrase (without wake word) should be ignored
    """
    # This is a documentation test showing expected behavior
    expected_behavior = {
        "first_phrase": {
            "contains": ["peter", "coffee", "cup"],
            "excludes": ["cake", "slice"],
            "should_route": True,
            "target": "peter_target"
        },
        "second_phrase": {
            "contains": ["also", "cake", "slice"],
            "excludes": ["peter"],
            "should_route": False,  # No wake word
            "reason": "5-second silence gap ended first session"
        }
    }
    
    print("ðŸ“‹ Expected wake word behavior:")
    print(f"   First phrase: {expected_behavior['first_phrase']}")
    print(f"   Second phrase: {expected_behavior['second_phrase']}")
    
    # This test passes to document expected behavior
    assert True
